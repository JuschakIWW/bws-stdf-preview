# Project name
COMPOSE_PROJECT_NAME="stdf-tool"

# ORION_URL corresponds to the container name for DNS lookup
ORION_URL = "orion"
ORION_PORT="1026"
ORION_VERSION="3.6.0"

# Core-tool variables
CORE_PORT="5000"

# MongoDB variables
MONGO_DATABASE="stdf_db"
MONGO_DB_PORT="27017"
MONGO_DB_VERSION="5.0"
MONGO_ROOT_USERNAME="root"
MONGO_ROOT_PASSWORD="default-password"

# DB related variables of the core tool (names of MongoDB collections)
# Name of collection with meta data on the physical meters / devices (accessed but not managed by the tool)
PHYSICAL_METER_COLLECTION="devices"

# Name of collection with meta data on the virtual meters (managed by the tool)
VIRTUAL_METER_COLLECTION="virtualDevices"

# Name of collection with measurement data of the physical meters defined in PHYSICAL_METER_COLLECTION (accessed but not managed by the tool)
PHYSICAL_METER_MEASUREMENT_COLLECTION="deviceMeasurements"

# Name of the collection that stores the meta data of the trained ML models (managed by the tool)
ML_MODEL_COLLECTION="mlModels"


###### Constants to determine holiday dates ######
# Read more here: https://pypi.org/project/holidays/
COUNTRY_CODE="DE"
SUBDIVISION_CODE=


###### Models ######
# The algorithm to use for training if none is specified in the API call
DEFAULT_ALGORITHM="xgboost"

# The unit of measurement the consumption data is stored as in the database
# In case of use of an acronym use units accepted in CEFACT code: https://unece.org/trade/uncefact
# MQH corresponds to cubic metre per hour (mÂ³/h)
DEFAULT_MEASUREMENT_UNIT="MQH"

# Maximum number of epochs to train torch-based models (i.e. deep-learning models)
N_EPOCHS_TORCH=100


###### Ray settings for hyperparameter tuning ######
# The number of configurations to test if not specified by the user
RAY_N_MODELS=16

# Maximum number of models that are trained concurrently
RAY_MAX_CONCURRENT=4

# The reduction factor to be applied in Hyperband after a set of models have been trained
# A value of 2 halves the number of configurations to train in the next step and doubles the
# computation budget for each of the remaining configurations
RAY_REDUCTION_FACTOR=2

# Number of CPUs to use for hyperparameter search
RAY_NUM_CPUS=8

# Number of GPUs to use for hyperparameter search
RAY_NUM_GPUS=0

# Maximum number of iterations (usually epochs) to train a model
# Difference to N_EPOCHS_TORCH is that RAY_MAX_T is used for trials during hyperparameter tuning
# whereas N_EPOCHS_TORCH is used for training and evaluation of the final model.
RAY_MAX_T=100
